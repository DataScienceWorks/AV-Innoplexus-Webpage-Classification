{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tnrange\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>isrctn.com</td>\n",
       "      <td>http://www.isrctn.com/ISRCTN57801413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>www.clinicaltrialsregister.eu</td>\n",
       "      <td>https://www.clinicaltrialsregister.eu/ctr-sear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>www.clinicaltrialsregister.eu</td>\n",
       "      <td>https://www.clinicaltrialsregister.eu/ctr-sear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>www.clinicaltrialsregister.eu</td>\n",
       "      <td>https://www.clinicaltrialsregister.eu/ctr-sear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>www.clinicaltrialsregister.eu</td>\n",
       "      <td>https://www.clinicaltrialsregister.eu/ctr-sear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Webpage_id                         Domain  \\\n",
       "0          31                     isrctn.com   \n",
       "1          32  www.clinicaltrialsregister.eu   \n",
       "2          33  www.clinicaltrialsregister.eu   \n",
       "3          34  www.clinicaltrialsregister.eu   \n",
       "4          35  www.clinicaltrialsregister.eu   \n",
       "\n",
       "                                                 Url  \n",
       "0               http://www.isrctn.com/ISRCTN57801413  \n",
       "1  https://www.clinicaltrialsregister.eu/ctr-sear...  \n",
       "2  https://www.clinicaltrialsregister.eu/ctr-sear...  \n",
       "3  https://www.clinicaltrialsregister.eu/ctr-sear...  \n",
       "4  https://www.clinicaltrialsregister.eu/ctr-sear...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_data=pd.read_csv(\"train.csv\")\n",
    "tst_data=pd.read_csv(\"test.csv\")\n",
    "tst_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webpage_id</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Url</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/marketing/tecfider...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/pharma/novo-equipp...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/pharma/another-exe...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/pharma/teva-buy-bi...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>www.fiercepharma.com</td>\n",
       "      <td>http://www.fiercepharma.com/marketing/actress-...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Webpage_id                Domain  \\\n",
       "0           1  www.fiercepharma.com   \n",
       "1           2  www.fiercepharma.com   \n",
       "2           3  www.fiercepharma.com   \n",
       "3           4  www.fiercepharma.com   \n",
       "4           5  www.fiercepharma.com   \n",
       "\n",
       "                                                 Url   Tag  \n",
       "0  http://www.fiercepharma.com/marketing/tecfider...  news  \n",
       "1  http://www.fiercepharma.com/pharma/novo-equipp...  news  \n",
       "2  http://www.fiercepharma.com/pharma/another-exe...  news  \n",
       "3  http://www.fiercepharma.com/pharma/teva-buy-bi...  news  \n",
       "4  http://www.fiercepharma.com/marketing/actress-...  news  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "others            17417\n",
       "news               7992\n",
       "publication        7705\n",
       "profile            5196\n",
       "conferences        4666\n",
       "forum              4503\n",
       "clinicalTrials     2839\n",
       "thesis             1800\n",
       "guidelines         1329\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_data.Tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=tra_data.shape[0]+tst_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile util.py\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r=re.compile('<p.+/p>')\n",
    "r2=re.compile('>(.*?)<')\n",
    "sep='!@#$%^&*()'\n",
    "\n",
    "def write_pages_text(k,beg,end):\n",
    "    with open('pages%d.txt'%k,'w',encoding='utf-8') as w:\n",
    "        for i in range(beg,end):\n",
    "            with open('pages/%d.html'%(i+1),encoding='utf-8') as o:\n",
    "                p='.'.join([t.strip() for t in BeautifulSoup(o).get_text(sep).split(sep) if t.find('{')==-1 and len(t.strip())>0])\n",
    "    #             print(p)\n",
    "                w.write(p.replace('\\n','')+'\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# from util import write_pages_text\n",
    "\n",
    "# procs=4\n",
    "# rng=size//procs\n",
    "# for i in range(procs):\n",
    "#     if i < procs-1:\n",
    "#         p=multiprocessing.Process(target=write_pages_text, args=(i+1,i*rng,(i+1)*rng))\n",
    "#     else:\n",
    "#         p=multiprocessing.Process(target=write_pages_text, args=(i+1,i*rng,size))\n",
    "#     p.start()\n",
    "# ! cat pages1.txt pages2.txt pages3.txt pages4.txt > pages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370099"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_words():\n",
    "    with open('words_alpha.txt') as word_file:\n",
    "        valid_words = set(word_file.read().split())\n",
    "\n",
    "    return valid_words\n",
    "words_dict=load_words()\n",
    "len(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce3bc1da68e4bc089d39457a8a6dceb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('pages.txt',encoding='utf-8') as o:\n",
    "    lines=o.readlines()\n",
    "    table=str.maketrans('','','!\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    for i in tnrange(len(lines)):\n",
    "        l = lines[i]\n",
    "        words = [w.translate(table).lower() for w in l.split() if w.translate(table).lower() in words_dict]\n",
    "        lines[i]=' '.join(words)\n",
    "    pages=[lines[i-1] for i in tra_data.Webpage_id]\n",
    "    pages_tst=[lines[i-1] for i in tst_data.Webpage_id]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53447 9 5938 466.0 114.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "pages_n=tra_data.shape[0]\n",
    "classes_n=tra_data.Tag.nunique()\n",
    "pages_per_cls=pages_n//classes_n\n",
    "\n",
    "def get_num_words_per_page(page_texts):\n",
    "    \"\"\"Returns the median number of words per page given corpus.\n",
    "\n",
    "    # Arguments\n",
    "        page_texts: list, page texts.\n",
    "\n",
    "    # Returns\n",
    "        int, median number of words per page.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in page_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "def plot_page_length_distribution(page_texts):\n",
    "    \"\"\"Plots the page length distribution.\n",
    "\n",
    "    # Arguments\n",
    "        pages_texts: list, page texts.\n",
    "    \"\"\"\n",
    "    lens=[len(s.split()) for s in page_texts]\n",
    "    l,u=np.percentile(lens,[0,95])\n",
    "    plt.hist(np.clip(lens,l,u), 200)\n",
    "    plt.xlabel('Length of a page')\n",
    "    plt.ylabel('Number of pages')\n",
    "    plt.title('page length distribution')\n",
    "    plt.show()\n",
    "    \n",
    "words_per_page=get_num_words_per_page(pages)\n",
    "\n",
    "print(pages_n,classes_n,pages_per_cls,words_per_page,pages_n//words_per_page)\n",
    "plot_page_length_distribution(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "C:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\sklearn\\utils\\__init__.py:93: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts, tst_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    x_tst = vectorizer.transform(tst_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    x_tst = selector.transform(x_tst).astype('float32')\n",
    "    return x_train, x_val, x_tst\n",
    "\n",
    "label_coder=LabelEncoder()\n",
    "tra_data['label']=label_coder.fit_transform(tra_data.Tag)\n",
    "\n",
    "pages_tra,pages_val,y_tra,y_val=train_test_split(pages,tra_data.label,test_size=0.1)\n",
    "\n",
    "x_tra_ngram,x_val_ngram,x_tst_ngram=ngram_vectorize(pages_tra,y_tra,pages_val,pages_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_24 (Dropout)         (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                1280064   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 1,280,649\n",
      "Trainable params: 1,280,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48102 samples, validate on 5345 samples\n",
      "Epoch 1/1000\n",
      " - 22s - loss: 1.3459 - acc: 0.5920 - val_loss: 0.9978 - val_acc: 0.6939\n",
      "Epoch 2/1000\n",
      " - 21s - loss: 0.9573 - acc: 0.7055 - val_loss: 0.8654 - val_acc: 0.7285\n",
      "Epoch 3/1000\n",
      " - 21s - loss: 0.8461 - acc: 0.7380 - val_loss: 0.8011 - val_acc: 0.7471\n",
      "Epoch 4/1000\n",
      " - 21s - loss: 0.7854 - acc: 0.7529 - val_loss: 0.7596 - val_acc: 0.7527\n",
      "Epoch 5/1000\n",
      " - 21s - loss: 0.7404 - acc: 0.7629 - val_loss: 0.7347 - val_acc: 0.7587\n",
      "Epoch 6/1000\n",
      " - 21s - loss: 0.7033 - acc: 0.7750 - val_loss: 0.7147 - val_acc: 0.7648\n",
      "Epoch 7/1000\n",
      " - 21s - loss: 0.6757 - acc: 0.7805 - val_loss: 0.6989 - val_acc: 0.7699\n",
      "Epoch 8/1000\n",
      " - 21s - loss: 0.6512 - acc: 0.7850 - val_loss: 0.6885 - val_acc: 0.7727\n",
      "Epoch 9/1000\n",
      " - 21s - loss: 0.6321 - acc: 0.7935 - val_loss: 0.6785 - val_acc: 0.7747\n",
      "Epoch 10/1000\n",
      " - 21s - loss: 0.6079 - acc: 0.7977 - val_loss: 0.6738 - val_acc: 0.7762\n",
      "Epoch 11/1000\n",
      " - 21s - loss: 0.5909 - acc: 0.8015 - val_loss: 0.6634 - val_acc: 0.7796\n",
      "Epoch 12/1000\n",
      " - 21s - loss: 0.5744 - acc: 0.8058 - val_loss: 0.6598 - val_acc: 0.7794\n",
      "Epoch 13/1000\n",
      " - 21s - loss: 0.5567 - acc: 0.8096 - val_loss: 0.6572 - val_acc: 0.7832\n",
      "Epoch 14/1000\n",
      " - 21s - loss: 0.5481 - acc: 0.8147 - val_loss: 0.6571 - val_acc: 0.7845\n",
      "Epoch 15/1000\n",
      " - 21s - loss: 0.5312 - acc: 0.8189 - val_loss: 0.6534 - val_acc: 0.7848\n",
      "Epoch 16/1000\n",
      " - 21s - loss: 0.5198 - acc: 0.8214 - val_loss: 0.6540 - val_acc: 0.7807\n",
      "Epoch 17/1000\n",
      " - 21s - loss: 0.5095 - acc: 0.8237 - val_loss: 0.6514 - val_acc: 0.7867\n",
      "Epoch 18/1000\n",
      " - 21s - loss: 0.4978 - acc: 0.8279 - val_loss: 0.6520 - val_acc: 0.7871\n",
      "Epoch 19/1000\n",
      " - 21s - loss: 0.4859 - acc: 0.8312 - val_loss: 0.6498 - val_acc: 0.7869\n",
      "Epoch 20/1000\n",
      " - 21s - loss: 0.4781 - acc: 0.8322 - val_loss: 0.6557 - val_acc: 0.7865\n",
      "Epoch 21/1000\n",
      " - 21s - loss: 0.4666 - acc: 0.8385 - val_loss: 0.6549 - val_acc: 0.7860\n",
      "Validation accuracy: 0.7859681942732787, loss: 0.6548903039619117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model\n",
    "\n",
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.4):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    x_train, train_labels, x_val, val_labels = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = train_labels.nunique()\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "#     x_train, x_val = ngram_vectorize(\n",
    "#         train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "    model.summary()\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    h = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = h.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    return model\n",
    "\n",
    "word_bag_mlp=train_ngram_model((x_tra_ngram,y_tra,x_val_ngram,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(model,x_tst,name):\n",
    "    y_pred=model.predict_classes(x_tst)\n",
    "    results=tst_data[['Webpage_id']].copy()\n",
    "    results['Tag']=[label_coder.classes_[c] for c in y_pred]\n",
    "\n",
    "    results.to_csv('%s.csv'%name,index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "others            9660\n",
       "publication       4282\n",
       "news              3710\n",
       "profile           2259\n",
       "conferences       2202\n",
       "forum             2138\n",
       "clinicalTrials     801\n",
       "thesis             437\n",
       "guidelines         298\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=submit(word_bag_mlp,x_tst_ngram,'word_bag_mlp_drop04')\n",
    "results.Tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84278"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts, tst_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "    x_tst = tokenizer.texts_to_sequences(tst_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    x_tst = sequence.pad_sequences(x_tst, maxlen=max_length)\n",
    "    return x_train, x_val, x_tst, tokenizer.word_index\n",
    "\n",
    "x_tra_seq,x_val_seq,x_tst_seq,word_index=sequence_vectorize(pages_tra,pages_val,pages_tst)\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6f9bb07f0e4e8eb184133222eb8ee5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r',encoding='utf-8')\n",
    "    model = {}\n",
    "    for _ in tnrange(int(400000)):\n",
    "        splitLine = f.readline().split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    return model\n",
    "\n",
    "EMBEDDING_DIM=200\n",
    "glove_model=loadGloveModel('glove.6B.%dd.txt'%EMBEDDING_DIM)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 500, 200)          16855800  \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 500, 200)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_29 (Separab (None, 500, 128)          26328     \n",
      "_________________________________________________________________\n",
      "separable_conv1d_30 (Separab (None, 500, 128)          16896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_31 (Separab (None, 250, 256)          33408     \n",
      "_________________________________________________________________\n",
      "separable_conv1d_32 (Separab (None, 250, 256)          66560     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_8 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 17,001,305\n",
      "Trainable params: 145,505\n",
      "Non-trainable params: 16,855,800\n",
      "_________________________________________________________________\n",
      "Train on 48102 samples, validate on 5345 samples\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-7eba8557e1f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m \u001b[0mseq_cnn_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_cnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tra_seq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_tra\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_val_seq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_cnn_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_tst_seq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'seq_cnn_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-7eba8557e1f2>\u001b[0m in \u001b[0;36mtrain_cnn_model\u001b[1;34m(data, learning_rate, epochs, batch_size, dropout_rate)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Logs once per epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;31m# Print results.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1214\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    243\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2822\u001b[0m     \u001b[0mfetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2823\u001b[0m     updated = session.run(\n\u001b[1;32m-> 2824\u001b[1;33m         fetches=fetches, feed_dict=feed_dict, **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2825\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "\n",
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model\n",
    "\n",
    "def train_cnn_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=16,\n",
    "                      dropout_rate=0.2):\n",
    "\n",
    "    x_train, train_labels, x_val, val_labels = data\n",
    "    num_classes = train_labels.nunique()\n",
    "    \n",
    "    # Create model instance.\n",
    "    model = sepcnn_model(2,128,3,EMBEDDING_DIM,0.2,2,(x_train.shape[1],),num_classes,len(word_index)+1,True,False,embedding_matrix)\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    h = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = h.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "\n",
    "seq_cnn_model=train_cnn_model((x_tra_seq,y_tra,x_val_seq,y_val),)\n",
    "submit(seq_cnn_model,x_tst_seq,'seq_cnn_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
