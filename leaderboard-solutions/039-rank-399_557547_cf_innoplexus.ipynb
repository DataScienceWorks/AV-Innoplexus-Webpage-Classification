{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string, re, io\n",
    "import eli5\n",
    "\n",
    "% matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainTestHelper(object):\n",
    "    def __init__(self):\n",
    "        self.ntrain = None\n",
    "\n",
    "    def combine(self, train, test):\n",
    "        self.ntrain = train.shape[0]\n",
    "        if isinstance(train, np.ndarray):\n",
    "            return np.row_stack((train, test))\n",
    "        else:\n",
    "            return train.append(test, sort=False).reset_index(drop=True)\n",
    "\n",
    "    def split(self, train_test):\n",
    "        if self.ntrain is None:\n",
    "            return None\n",
    "        if isinstance(train_test, np.ndarray):\n",
    "            train = train_test[:self.ntrain, :]\n",
    "            test = train_test[self.ntrain:, :]\n",
    "        else:\n",
    "            train = train_test.iloc[:self.ntrain, :].copy().reset_index(drop=True)\n",
    "            test = train_test.iloc[self.ntrain:, :].copy().reset_index(drop=True)\n",
    "        return train, test\n",
    "    \n",
    "def clean_text(x):\n",
    "    w = ''\n",
    "    for i in x:\n",
    "        if i in string.punctuation:\n",
    "            w += ' '\n",
    "        else:\n",
    "            w += i\n",
    "    return w\n",
    "\n",
    "def count_regexp_occ(regexp='', text=None):\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "def CBOW(x, vec):\n",
    "    w = np.zeros(300)\n",
    "    for i in x:\n",
    "        try:\n",
    "            w += vec[i]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test_nvPHrOx.csv')\n",
    "ss = pd.read_csv('sample_submission_poy1UIu.csv')\n",
    "\n",
    "helper = TrainTestHelper()\n",
    "data = helper.combine(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['ndot_domain'] = data['Domain'].apply(lambda x: str(x).count('.'))\n",
    "data['ndot_url'] = data['Url'].apply(lambda x: str(x).count('.'))\n",
    "data['nslash_url'] = data['Url'].apply(lambda x: str(x).count('/'))\n",
    "data['mail_url'] = data['Url'].apply(lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x))\n",
    "data['nupper_url'] = data['Url'].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "data['time_url'] = data['Url'].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "data['date_long_url'] = data['Url'].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "data['date_short_url'] = data['Url'].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "data['nchick_url'] = data['Url'].apply(lambda x: x.count(\"!\"))\n",
    "data['nqmark_url'] = data['Url'].apply(lambda x: x.count(\"?\"))\n",
    "data['http_vs_https'] = data['Url'].apply(lambda x: str(x).startswith('http://')).astype(int)\n",
    "data['ndigit_domain'] = data['Domain'].apply(lambda x: sum([c.isdigit() for c in str(x)]))\n",
    "data['ndigit_url'] = data['Url'].apply(lambda x: sum([c.isdigit() for c in str(x)]))                             \n",
    "\n",
    "data['Url'] = data['Url'].apply(lambda x: str(x).replace('https://', '').replace('http://', ''))\n",
    "data['Domain_nopunct'] = data['Domain'].apply(lambda x: clean_text(x))\n",
    "data['Url_nopunct'] = data['Url'].apply(lambda x: clean_text(x))\n",
    "data['all_text'] = data['Domain_nopunct'] + ' ' + data['Url_nopunct']\n",
    "data['len_url'] = data['Url'].apply(len)\n",
    "data['len_domain'] = data['Domain'].apply(len)\n",
    "data['nword_url'] = data['Url_nopunct'].apply(lambda x: len(x.split(' ')))\n",
    "data['nword_domain'] = data['Url_nopunct'].apply(lambda x: len(x.split(' ')))\n",
    "data['rdigit_domain'] = data['ndigit_domain'] / data['len_domain']\n",
    "data['rdigit_url'] = data['ndigit_url'] / data['len_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['all_text', 'ndot_domain', 'ndot_url', 'nslash_url', 'mail_url', 'nupper_url', 'time_url', 'date_long_url',\n",
    "        'date_short_url', 'nchick_url', 'nqmark_url', 'http_vs_https', 'len_url', 'len_domain', 'nword_url',\n",
    "        'nword_domain', 'ndigit_domain', 'ndigit_url', 'rdigit_domain', 'rdigit_url', 'Tag', 'Domain']\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train, Test = helper.split(data)\n",
    "le = LabelEncoder()\n",
    "le.fit(Train['Tag'])\n",
    "Train = Train.sort_values(by='Domain').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = Train.loc[:25009, :]\n",
    "valid = Train.loc[25010:, :].reset_index(drop=True)\n",
    "y_train, y_valid = le.transform(train['Tag']), le.transform(valid['Tag'])\n",
    "y = le.transform(Train['Tag'])\n",
    "del Train['Tag'], Test['Tag'], Train['Domain'], Test['Domain'], train['Tag'], valid['Tag'], train['Domain'], valid['Domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), max_features=200, min_df=5)\n",
    "char = TfidfVectorizer(ngram_range=(2, 4), max_features=500, analyzer='char')\n",
    "tfidf.fit(data['all_text'])\n",
    "char.fit(data['all_text'])\n",
    "\n",
    "train_tfidf = tfidf.transform(train['all_text'])\n",
    "valid_tfidf = tfidf.transform(valid['all_text'])\n",
    "train_char = char.transform(train['all_text'])\n",
    "valid_char = char.transform(valid['all_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_feats = ['ndot_url', 'time_url', 'nqmark_url', 'ndigit_domain', 'rdigit_domain', 'rdigit_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = hstack([train[num_feats], train_tfidf, train_char])\n",
    "valid_features = hstack([valid[num_feats], valid_tfidf, valid_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', C=0.5, random_state=0)\n",
    "lr.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6204493460655778\n"
     ]
    }
   ],
   "source": [
    "valid_pred = lr.predict(valid_features)\n",
    "print('F1 score:', f1_score(y_valid, valid_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_tfidf = tfidf.transform(Train['all_text'])\n",
    "Test_tfidf = tfidf.transform(Test['all_text'])\n",
    "Train_char = char.transform(Train['all_text'])\n",
    "Test_char = char.transform(Test['all_text'])\n",
    "\n",
    "Train_features = hstack([Train[num_feats], Train_tfidf, Train_char])\n",
    "Test_features = hstack([Test[num_feats], Test_tfidf, Test_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', C=0.5, random_state=0)\n",
    "lr.fit(Train_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "ss['Tag'] = le.inverse_transform(lr.predict(Test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss.to_csv('lr_0.62CV.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
