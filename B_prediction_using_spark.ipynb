{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc not defined\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    print(\"sc not defined\")\n",
    "\n",
    "config = SparkConf().setMaster(\"local[*]\").setAppName(\"ClassifyUrl\")    \n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Classify Urls\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = 'bigdata/train.csv'\n",
    "html_csv = 'bigdata/train/html_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Webpage_id: integer (nullable = true)\n",
      " |-- Domain: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Tag: string (nullable = true)\n",
      "\n",
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = spark.read.csv(train_csv, \n",
    "                       header=True,\n",
    "                      inferSchema=True)\n",
    "\n",
    "# Analyze Data types in dataset\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records in Training Dataset : 53447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Webpage_id=1, Domain='www.fiercepharma.com', Url='http://www.fiercepharma.com/marketing/tecfidera-gilenya-and-aubagio-s-3-way-battle-for-ms-share-about-to-get-more-interesting', Tag='news'),\n",
       " Row(Webpage_id=2, Domain='www.fiercepharma.com', Url='http://www.fiercepharma.com/pharma/novo-equipped-to-weather-storm-u-s-diabetes-market-ceo-says', Tag='news'),\n",
       " Row(Webpage_id=3, Domain='www.fiercepharma.com', Url='http://www.fiercepharma.com/pharma/another-exec-departs-troubled-endo-and-time-it-s-for-another-drugmaker', Tag='news'),\n",
       " Row(Webpage_id=4, Domain='www.fiercepharma.com', Url='http://www.fiercepharma.com/pharma/teva-buy-biosim-specialist-celltrion-it-wouldn-t-say-no', Tag='news'),\n",
       " Row(Webpage_id=5, Domain='www.fiercepharma.com', Url='http://www.fiercepharma.com/marketing/actress-marissa-tomei-partners-allergan-restasis-to-drive-dry-eye-awareness', Tag='news')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total Records in Training Dataset :', train.count())\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----+\n",
      "|Webpage_id|              Domain|                 Url| Tag|\n",
      "+----------+--------------------+--------------------+----+\n",
      "|         1|www.fiercepharma.com|http://www.fierce...|news|\n",
      "|         2|www.fiercepharma.com|http://www.fierce...|news|\n",
      "|         3|www.fiercepharma.com|http://www.fierce...|news|\n",
      "|         4|www.fiercepharma.com|http://www.fierce...|news|\n",
      "|         5|www.fiercepharma.com|http://www.fierce...|news|\n",
      "+----------+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# head() gives Ugly Output :(\n",
    "# Prefer show() over head()\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of cols in train dataset :  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Webpage_id', 'Domain', 'Url', 'Tag']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many columns do we have in train and what are their names?\n",
    "print('No. of cols in train dataset : ', len(train.columns))\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------+\n",
      "|summary|        Webpage_id|              Domain|                 Url|           Tag|\n",
      "+-------+------------------+--------------------+--------------------+--------------+\n",
      "|  count|             53447|               53447|               53447|         53447|\n",
      "|   mean| 39920.78603102139|                null|                null|          null|\n",
      "| stddev|22945.942450142324|                null|                null|          null|\n",
      "|    min|                 1|  1.eyefortravel.com|http://1.eyefortr...|clinicalTrials|\n",
      "|    max|             79345|zoonosis.conferen...|https://zoologica...|        thesis|\n",
      "+-------+------------------+--------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nObservation:\\nAs we can see that, describe operation is working for String type column but the output for mean, stddev are null and min & max values are calculated based on ASCII value of categories.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame?\n",
    "train.describe().show()\n",
    "\n",
    "'''\n",
    "Observation:\n",
    "As we can see that, describe operation is working for String type column but the output for mean, stddev are null and min & max values are calculated based on ASCII value of categories.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+\n",
      "|Webpage_id|              Domain| Tag|\n",
      "+----------+--------------------+----+\n",
      "|         1|www.fiercepharma.com|news|\n",
      "|         2|www.fiercepharma.com|news|\n",
      "|         3|www.fiercepharma.com|news|\n",
      "|         4|www.fiercepharma.com|news|\n",
      "|         5|www.fiercepharma.com|news|\n",
      "+----------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 804 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# How to select column(s) from the DataFrame?\n",
    "train.select('Webpage_id','Domain','Tag').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3974, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# How to find the number of distinct Domain and Tags (Target-Classes) in train files?\n",
    "train.select('Domain').distinct().count(), train.select('Tag').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Missing values in Train Dataset :\n",
      "\n",
      "Domain : 0\n",
      "Url : 0\n",
      "Tag : 0\n"
     ]
    }
   ],
   "source": [
    "# Check for Null values in \n",
    "print('Count of Missing values in Train Dataset :\\n')\n",
    "print('Domain :', train.filter(train.Domain.isNull()).count())\n",
    "print('Url :', train.filter(train.Url.isNull()).count())\n",
    "print('Tag :', train.filter(train.Tag.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53447"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to drop the all rows with null value?\n",
    "train.dropna().count() # Count of rows in newly returned non-null dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53447"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to fill the null values in Domain column of DataFrame with, some constant value, say, 'www.missing.in'?\n",
    "missing_domain = 'www.missing.in'\n",
    "train.fillna(missing_domain, 'Domain').count() # Count of rows in newly returned non-null dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------+\n",
      "|Webpage_id|              Domain|                 Url|        Tag|\n",
      "+----------+--------------------+--------------------+-----------+\n",
      "|     18472|cancerci.biomedce...|https://cancerci....|publication|\n",
      "|     26364|www.naturalnews.c...|http://www.natura...|       news|\n",
      "|     28191|         twitter.com|https://twitter.c...|     others|\n",
      "+----------+--------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to create a sample DataFrame from the base DataFrame?\n",
    "train.sample(False, # withReplacement=False\n",
    "             0.0001, # fraction = x percecntage that we want to pick\n",
    "             42 # seed to reproduce the result\n",
    "            ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              Domain|count|\n",
      "+--------------------+-----+\n",
      "|thesis.library.ca...|  301|\n",
      "|academiccommons.c...|  300|\n",
      "|  www.dart-europe.eu|  300|\n",
      "|       curate.nd.edu|  300|\n",
      "|      dspace.mit.edu|  300|\n",
      "|ecommons.cornell.edu|  300|\n",
      "|     www.nice.org.uk|  230|\n",
      "|www.ncbi.nlm.nih.gov|  226|\n",
      "+--------------------+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|              Domain|count|\n",
      "+--------------------+-----+\n",
      "|thesis.library.ca...|  301|\n",
      "|academiccommons.c...|  300|\n",
      "|  www.dart-europe.eu|  300|\n",
      "|       curate.nd.edu|  300|\n",
      "|      dspace.mit.edu|  300|\n",
      "|ecommons.cornell.edu|  300|\n",
      "|     www.nice.org.uk|  230|\n",
      "|www.ncbi.nlm.nih.gov|  226|\n",
      "+--------------------+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|              Domain|count|\n",
      "+--------------------+-----+\n",
      "|thesis.library.ca...|  301|\n",
      "|academiccommons.c...|  300|\n",
      "|  www.dart-europe.eu|  300|\n",
      "|       curate.nd.edu|  300|\n",
      "|      dspace.mit.edu|  300|\n",
      "|ecommons.cornell.edu|  300|\n",
      "|     www.nice.org.uk|  230|\n",
      "|www.ncbi.nlm.nih.gov|  226|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to find the number of rows we have per Domain?\n",
    "from pyspark.sql.functions import col\n",
    "train.groupBy('Domain') \\\n",
    "    .count() \\\n",
    "    .filter(\"`count` > 225\") \\\n",
    "    .sort(col('count').desc()) \\\n",
    "    .show(10) # Show Count of Top 10\n",
    "\n",
    "# Or like below:\n",
    "from pyspark.sql.functions import desc\n",
    "train.groupBy('Domain') \\\n",
    "    .count() \\\n",
    "    .filter(\"`count` > 225\") \\\n",
    "    .sort(desc('count')) \\\n",
    "    .show(10) # Show Count of Top 10\n",
    "\n",
    "# Or like below:\n",
    "from pyspark.sql.functions import desc\n",
    "train.groupBy('Domain') \\\n",
    "    .count() \\\n",
    "    .filter(\"`count` > 225\") \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .show(10) # Show Count of Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+----+\n",
      "|Webpage_id|      Domain|                 Url| Tag|\n",
      "+----------+------------+--------------------+----+\n",
      "|         1|fiercepharma|http://www.fierce...|news|\n",
      "|         2|fiercepharma|http://www.fierce...|news|\n",
      "|         3|fiercepharma|http://www.fierce...|news|\n",
      "|         4|fiercepharma|http://www.fierce...|news|\n",
      "|         5|fiercepharma|http://www.fierce...|news|\n",
      "+----------+------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Wall time: 2.13s - 2.39s\n",
    "\n",
    "# OBJECTIVE : Get just the domain from URLs\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import tldextract\n",
    "\n",
    "def extract_domain(url):\n",
    "    return tldextract.extract(url).domain\n",
    "\n",
    "extract_domain_udf = udf(extract_domain, StringType())\n",
    "# extract_domain_udf = udf(lambda url : tldextract.extract(url).domain, StringType())\n",
    "\n",
    "train = train.withColumn('Domain', extract_domain_udf(train.Domain))\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For Scraping HTML page\n",
    "from bs4.element import Comment\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective : Extract text from title tag of HTML source of web-page\n",
    "def extract_title(page):\n",
    "    if (page == None): \n",
    "        return None\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    title_tag = soup.find('title')\n",
    "    if (title_tag == None):\n",
    "        title = None\n",
    "    else:\n",
    "        title = title_tag.text.strip()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  OBJECTIVE: Functions to parse HTML content and extract text that matters.\n",
    "def extract_body(page):\n",
    "    if (page == None): \n",
    "        return None\n",
    "    soup = BeautifulSoup(page, 'html.parser', from_encoding=\"utf-8\")\n",
    "    body_tag = soup.find('body')\n",
    "    if (body_tag == None):\n",
    "        body = page \n",
    "    else:\n",
    "        body = body_tag # What should be returned here? How to stringify this for further  procecssing?\n",
    "    return body\n",
    "\n",
    "def is_visible_content(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def remove_extra_spaces(str):\n",
    "    return u\" \".join(str.split())\n",
    "\n",
    "def extract_text(page):\n",
    "    if (page == None): \n",
    "        return None\n",
    "    soup = BeautifulSoup(page, 'html.parser') #, from_encoding=\"utf-8\"\n",
    "    texts = soup.findAll(text=True) # Extracts text from all HTML Markups, incl nested ones\n",
    "    visible_texts = filter(is_visible_content, texts)\n",
    "    # The u-prefix u\" \".join() indicates Unicode and has been in python since v2.0\n",
    "    # Ref. Read: https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\n",
    "    text = u\" \".join(remove_extra_spaces(t.strip()) for t in visible_texts)\n",
    "    text = text.replace(',','')\n",
    "    text = text.replace('|','')\n",
    "    text = re.sub(r'\\s\\s+',' ',text).strip()\n",
    "    return text.encode('utf-8',errors='ignore').decode('utf-8').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o566.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 1080, localhost, executor driver): com.univocity.parsers.common.TextParsingException: java.lang.ArrayIndexOutOfBoundsException - null\nParser Configuration: CsvParserSettings:\n\tAuto configuration enabled=true\n\tAutodetect column delimiter=false\n\tAutodetect quotes=false\n\tColumn reordering enabled=true\n\tEmpty value=null\n\tEscape unquoted values=false\n\tHeader extraction enabled=null\n\tHeaders=null\n\tIgnore leading whitespaces=true\n\tIgnore trailing whitespaces=true\n\tInput buffer size=128\n\tInput reading on separate thread=false\n\tKeep escape sequences=false\n\tKeep quotes=false\n\tLength of content displayed on error=-1\n\tLine separator detection enabled=false\n\tMaximum number of characters per column=-1\n\tMaximum number of columns=2\n\tNormalize escaped line separators=true\n\tNull value=\n\tNumber of records to read=all\n\tProcessor=none\n\tRestricting data in exceptions=false\n\tRowProcessor error handler=null\n\tSelected fields=none\n\tSkip bits as whitespace=true\n\tSkip empty lines=true\n\tUnescaped quote handling=STOP_AT_DELIMITERFormat configuration:\n\tCsvFormat:\n\t\tComment character=\\0\n\t\tField delimiter=,\n\t\tLine separator (normalized)=\\n\n\t\tLine separator sequence=\\r\\n\n\t\tQuote character=\"\n\t\tQuote escape character=\\\n\t\tQuote escape escape character=null\nInternal state when error was thrown: line=3, column=3, record=3, charIndex=755, headers=[1, <!DOCTYPE html>]\r\n\tat com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:368)\r\n\tat com.univocity.parsers.common.AbstractParser.parseLine(AbstractParser.java:609)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$5$$anonfun$apply$2.apply(CSVDataSource.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$5$$anonfun$apply$2.apply(CSVDataSource.scala:169)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1116)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1116)\r\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)\r\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1118)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1111)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$.infer(CSVInferSchema.scala:44)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:171)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: com.univocity.parsers.common.TextParsingException: java.lang.ArrayIndexOutOfBoundsException - null\nParser Configuration: CsvParserSettings:\n\tAuto configuration enabled=true\n\tAutodetect column delimiter=false\n\tAutodetect quotes=false\n\tColumn reordering enabled=true\n\tEmpty value=null\n\tEscape unquoted values=false\n\tHeader extraction enabled=null\n\tHeaders=null\n\tIgnore leading whitespaces=true\n\tIgnore trailing whitespaces=true\n\tInput buffer size=128\n\tInput reading on separate thread=false\n\tKeep escape sequences=false\n\tKeep quotes=false\n\tLength of content displayed on error=-1\n\tLine separator detection enabled=false\n\tMaximum number of characters per column=-1\n\tMaximum number of columns=2\n\tNormalize escaped line separators=true\n\tNull value=\n\tNumber of records to read=all\n\tProcessor=none\n\tRestricting data in exceptions=false\n\tRowProcessor error handler=null\n\tSelected fields=none\n\tSkip bits as whitespace=true\n\tSkip empty lines=true\n\tUnescaped quote handling=STOP_AT_DELIMITERFormat configuration:\n\tCsvFormat:\n\t\tComment character=\\0\n\t\tField delimiter=,\n\t\tLine separator (normalized)=\\n\n\t\tLine separator sequence=\\r\\n\n\t\tQuote character=\"\n\t\tQuote escape character=\\\n\t\tQuote escape escape character=null\nInternal state when error was thrown: line=3, column=3, record=3, charIndex=755, headers=[1, <!DOCTYPE html>]\r\n\tat com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:368)\r\n\tat com.univocity.parsers.common.AbstractParser.parseLine(AbstractParser.java:609)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$5$$anonfun$apply$2.apply(CSVDataSource.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$5$$anonfun$apply$2.apply(CSVDataSource.scala:169)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1116)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1116)\r\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)\r\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o566.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 1080, localhost, executor driver): com.univocity.parsers.common.TextParsingException: java.lang.ArrayIndexOutOfBoundsException - null\nParser Configuration: CsvParserSettings:\n\tAuto configuration enabled=true\n\tAutodetect column delimiter=false\n\tAutodetect quotes=false\n\tColumn reordering enabled=true\n\tEmpty value=null\n\tEscape unquoted values=false\n\tHeader extraction enabled=null\n\tHeaders=null\n\tIgnore leading whitespaces=true\n\tIgnore trailing whitespaces=true\n\tInput buffer size=128\n\tInput reading on separate thread=false\n\tKeep escape sequences=false\n\tKeep quotes=false\n\tLength of content displayed on error=-1\n\tLine separator detection enabled=false\n\tMaximum number of characters per column=-1\n\tMaximum number of columns=2\n\tNormalize escaped line separators=true\n\tNull value=\n\tNumber of records to read=all\n\tProcessor=none\n\tRestricting data in exceptions=false\n\tRowProcessor error handler=null\n\tSelected fields=none\n\tSkip bits as whitespace=true\n\tSkip empty lines=true\n\tUnescaped quote handling=STOP_AT_DELIMITERFormat configuration:\n\tCsvFormat:\n\t\tComment character=\\0\n\t\tField delimiter=,\n\t\tLine separator (normalized)=\\n\n\t\tLine separator sequence=\\r\\n\n\t\tQuote character=\"\n\t\tQuote escape character=\\\n\t\tQuote escape escape character=null\nInternal state when error was thrown: line=3, column=3, record=3, charIndex=755, headers=[1, <!DOCTYPE html>]\r\n\tat com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:368)\r\n\tat com.univocity.parsers.common.AbstractParser.parseLine(AbstractParser.java:609)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$5$$anonfun$apply$2.apply(CSVDataSource.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$5$$anonfun$apply$2.apply(CSVDataSource.scala:169)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1116)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1116)\r\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)\r\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1118)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1111)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$.infer(CSVInferSchema.scala:44)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:171)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\r\n\tat scala.Option.orElse(Option.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: com.univocity.parsers.common.TextParsingException: java.lang.ArrayIndexOutOfBoundsException - null\nParser Configuration: CsvParserSettings:\n\tAuto configuration enabled=true\n\tAutodetect column delimiter=false\n\tAutodetect quotes=false\n\tColumn reordering enabled=true\n\tEmpty value=null\n\tEscape unquoted values=false\n\tHeader extraction enabled=null\n\tHeaders=null\n\tIgnore leading whitespaces=true\n\tIgnore trailing whitespaces=true\n\tInput buffer size=128\n\tInput reading on separate thread=false\n\tKeep escape sequences=false\n\tKeep quotes=false\n\tLength of content displayed on error=-1\n\tLine separator detection enabled=false\n\tMaximum number of characters per column=-1\n\tMaximum number of columns=2\n\tNormalize escaped line separators=true\n\tNull value=\n\tNumber of records to read=all\n\tProcessor=none\n\tRestricting data in exceptions=false\n\tRowProcessor error handler=null\n\tSelected fields=none\n\tSkip bits as whitespace=true\n\tSkip empty lines=true\n\tUnescaped quote handling=STOP_AT_DELIMITERFormat configuration:\n\tCsvFormat:\n\t\tComment character=\\0\n\t\tField delimiter=,\n\t\tLine separator (normalized)=\\n\n\t\tLine separator sequence=\\r\\n\n\t\tQuote character=\"\n\t\tQuote escape character=\\\n\t\tQuote escape escape character=null\nInternal state when error was thrown: line=3, column=3, record=3, charIndex=755, headers=[1, <!DOCTYPE html>]\r\n\tat com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:368)\r\n\tat com.univocity.parsers.common.AbstractParser.parseLine(AbstractParser.java:609)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$5$$anonfun$apply$2.apply(CSVDataSource.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$5$$anonfun$apply$2.apply(CSVDataSource.scala:169)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1116)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1116)\r\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)\r\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OBJECTIVE : Read html_data.csv\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import lit # lit for literals\n",
    "\n",
    "extract_text_udf = udf(extract_text, StringType())\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"Webpage_id\", IntegerType()),\n",
    "    StructField(\"Html\", StringType())\n",
    "])\n",
    "\n",
    "html_df = spark.read.csv(html_csv, \n",
    "                         header=True, \n",
    "                         ignoreLeadingWhiteSpace=True, \n",
    "                         ignoreTrailingWhiteSpace=True, \n",
    "                         encoding='utf-8', \n",
    "                         inferSchema=True, \n",
    "                         maxColumns=2)\n",
    "'''\n",
    "html_df = spark.read.format('csv') \\\n",
    "                    .option('header',True) \\\n",
    "                    .option('ignoreLeadingWhiteSpace',True) \\\n",
    "                    .option('ignoreTrailingWhiteSpace',True) \\\n",
    "                    .option('inferSchema',True) \\\n",
    "                    .option('maxColumns',2) \\\n",
    "                    .load(html_csv)\n",
    "'''\n",
    "# Analyze Data types in dataset\n",
    "html_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# OBJECTIVE : From html_data.csv loaded in df, extract title and text from html-page, and add the them to train.csv as new columns\n",
    "\n",
    "# Adding a constant column \n",
    "html_df = html_df.withColumn('Title',lit(''))\n",
    "html_df.show(5)\n",
    "\n",
    "# Transforming an existing column\n",
    "html_df = html_df.withColumn('Html',extract_text_udf(html_df.Html))\n",
    "html_df = html_df.withColumnRenamed('Html','Html2Text')\n",
    "html_df.write.csv('bigdata/train/spark1.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
